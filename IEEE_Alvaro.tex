\documentclass[conference, letterpaper]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{tabularx, booktabs} 
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\graphicspath{{./images/}}
\newcolumntype{C}{>{\centering\arraybackslash}p{1.8cm}}

\begin{document}

\title{Estimating Baseline Travel Times for the UK Strategic Road Network
%\thanks{Thanks to EPSRC and Thales UK.}
}

\author{\IEEEauthorblockN{Alvaro Cabrejas Egea}
\IEEEauthorblockA{\textit{MathSys Centre for Doctoral Training} \\
\textit{University of Warwick}\\
Coventry, United Kingdom \\
a.cabrejas-egea@warwick.ac.uk}
\and
\IEEEauthorblockN{Peter De Ford}
\IEEEauthorblockA{\textit{McKinsey \& Company} \\
	San Jose, Costa Rica \\
	pdeford@outlook.com}
\and
\IEEEauthorblockN{Colm Connaughton}
\IEEEauthorblockA{\textit{Warwick Mathematics Institute} \\
\textit{\& Centre for Complexity Science} \\
\textit{University of Warwick}\\
Coventry, United Kingdom \\
c.p.connaughton@warwick.ac.uk}
}

\maketitle

\begin{abstract}
We present a new method for long-term estimation of the expected travel time for links on highways and their variation with time.  
The approach is based on a time series analysis of travel time data from the UK's National Traffic Information Service (NTIS).  
Time series of travel times are characterised by a noisy background variation exhibiting the expected daily and weekly patterns punctuated by large spikes associated with congestion events. 
Some of these spikes are caused by peak hour congestion and some are caused by unforeseen events like accidents. 
Our algorithm uses thresholding to split the time series into background and spike signals, each of which is analysed separately. 
The the background signal is extracted using spectral filtering. 
The periodic part of the spike signal is extracted using locally weighted regression (LWR). 
The final estimated travel time is obtained by recombining these two. 
We assess our method by cross-validating in several UK motorways. 
We use 8 weeks of training data and calculate the mean absolute percentage error (MAPE) of the resulting travel time estimates for week of test data, repeating this process 4 times. 
We find that the MAPE is significantly reduced compared to estimates obtained by simple segmentation of the data and compared to the estimates published by the NTIS system.
\end{abstract}

\begin{IEEEkeywords}
Travel Time, Traffic Congestion, Prediction
\end{IEEEkeywords}

\section{Introduction}
The UK is considered world-leading in its ability to collect and process real-time data from its road network. 
Highways England are responsible for making this data available through the National Traffic Information Service (NTIS) \cite{NTIS}. 
Highways England collects speed, flow, travel time and other data on the Strategic Road Network using sensors on the road and in vehicles. 
The basic building blocks of the Strategic Road Network are the so-called links. 
They are segments of motorway, with a length varying between 500 and over 6000 meters. 
The links on the network incorporate thousands of induction loops at different sites, which report their measurements to a centralised system.\\

NTIS data is used to assign a traffic profile to each link on the network. 
Each profile contains the expected travel time at any given time of day, understood as the average time to transverse said link with a resolution of one minute.
Historical travel time data within the same link is used to compute expected future travel times.\\

Currently, generating these profiles makes use of a combination of an Exponentially Weighted Moving Average (EWMA) and heavy time-dependent segmentation (manual clustering), although the exact methodology is not made public. \\

Here, using NTIS travel time data, we present a novel method for generating profiles for a complete week, such that they do not require segmentation or an underlying model, only requiring statistical and spectral analysis of previous data.

The available literature on travel times is extensive but most recent research in focuses on short-term forecasting. From a methodological point of view, different approaches can be found .
*** HERE BE LIT REVIEW***

Most of these methods have been specifically tuned for the conditions on which they have been developed and transferability to other sites is often not evaluated.
To test the transferability of this method, it is tested on three different motorways.

\section{Examples of travel times} 
The most direct variable for measuring the state of traffic over a length of road is the vehicles' travel times. 
The instantaneous travel time for a given segment of road is the average time that the vehicles currently in it are taking since they enter the segment until they exit it.\\

\begin{figure}[htbp]
	\centerline{\includegraphics[width=\linewidth]{./images/Traveltime_example2.jpg}}
	\caption{Travel times on link 117007401 in the M6 over three weeks.}
	\label{fig:travel_time_example}
\end{figure}

As it can be seen in Figure \ref{fig:travel_time_example} the travel time remains within a vaguely predictable pattern most of the days with bounded minima during nights corresponding with the free flow time, with some outliers below this value corresponding to speeding drivers. 
Travel time will meaningfully rise as people leave to work and the add load to the motorways. 
This collective behaviour will create the morning traffic jams, which in our data, are partially replicated during the evening rush hour, normally finding a plateau in between. 
After this, the travel times slowly decay towards the night period of free-flow regime.\\

However, as it also follows from Figure \ref{fig:travel_time_example}, it can be seen that there are a series of excursions out of this otherwise oscillating yet bounded typical travel time. 
In these, the travel time can increase several times fold the usual values. These extreme oscillations are much less predictable both in intensity and inter-oscillation period than the recurrent congestion described previously.
 
\section{Current Traffic Profiles}
In this section, current approaches towards the generation of Traffic Profiles will be described, in line with how these are calculated in the United Kingdom.

\subsection{Use of Exponentially Weighted Moving Average} \label{ewma}
One currently used way for creating Traffic Profiles consists in applying an EWMA on the same minute of every day, with the implicit assumption that similar road behaviour can be expected at the same time of the day. 
Changes happening in a given time of the day, will be smooth enough across dates so their trend can be appropriately approximated by the EWMA.
In these algorithms the next Travel time forecast $\hat{x}(i,d+1)$ on the $i-th$ minute of a given day $d$, with measured travel time $x_i^d$, will be:
\begin{equation}
\hat{x}^{d+1}_i = \alpha * x^{d}_{i} + (1-\alpha)*\hat{x}^{d}_{i}
\end{equation}
The main problem when using an EWMA is the manner in which the memory decays. 
More recent measurements are weighted more heavily than events in the past, if an extreme fluctuation occurs, the following forecasts will be biased, partially replicating this event and over-estimating the travel times in that section of road until enough new measurements have been taken to dissipate this effect.

\subsection{Segmentation}\label{segmentation}
In addition, to acknowledge the specific differences between some special dates in the year, and the difference between days of the week, this family of methods requires the use of heavy date based segmentation. 
The EWMA will be applied across dates which fall in the same category (i.e. Mondays, weekends, Christmas Day, ...).
If this is combined with the shortcomings presented in Section \ref{ewma}, some long reaching effects are generated, which can propagate for weeks into the future predictions, but do not have any reflection on the observed travel times, as it can be seen in Figure \ref{fig:EWMA_spike}.
Furthermore, in order to generate a valid segmentation, an experienced team is necessary, since the needs of the process can geographically vary, given that the EWMA approach tends to better approximate endemic congestion. 
This dependence can lead to the creation of legacy systems which may not be well understood after a few years, decreasing their usability over time unless extra effort is put into transmitting this knowledge and continually train new staff.
\begin{figure}
	\includegraphics[width=\linewidth]{EWMA.pdf}
	\caption{Example of the current algorithm predicting increases in travel times for several weeks after a peak is seen in the measured travel times (25/04/2016)}
	\label{fig:EWMA_spike}
\end{figure}


\section{Data}
\subsection{Data Gathering and Selection}
Data was gathered from 3 different Motorways in England.
The M6 and M11 were selected due to their high usage and combination of recurrent and outstanding congestion. 
The M25 was selected on the base that it is the most used Motorway in England on a daily basis, suffering from chronic congestion.
\begin{itemize}
	\item The dataset for M6 comprises 90 days (12 complete weeks) of data (07/03/2016-05/06/2016) across 14 links.
	\item The dataset for d M11 comprises 90 days (12 complete weeks) of data (07/03/2016-05/06/2016) across 25 links.
	\item The dataset for M25 comprises 75 days (10 complete weeks) of data (07/04/2017-20/06/2017) across 61 links.
	\item Links were discarded if they had more than 10\% of missing data or if they contained any entry or exit ramps.
	\item Whenever missing data was detected for 10 or less minutes, it was linearly interpolated.
	\item Whenever missing data was detected for over 10 minutes, it was left as missing values.
\end{itemize}
In the case of the M6 and the M11 the first 8 weeks are used to predict one complete week ahead.
One week later, the process is repeated, deleting the oldest week of training data and incorporating measurements of the week predicted in the previous step. This is performed 4 times.
In the case of the M25, the dataset comprised 10 complete weeks. 
Hence, 6 weeks of data were used for training and the process as described above is performed 3 times.
\subsection{Data Contents}
For each link on a specific date, the required input data consists of one entry per minute, containing:
\begin{itemize}
	\item Measured travel time
	\item Profile (expected) travel time 	
\end{itemize}


\section{Travel Time Prediction Algorithm} \label{algorithm}

\begin{figure}[htbp]
	\centerline{\includegraphics[width=\linewidth]{./images/finalplots/ACF_M6_Link3.pdf}}
	\caption{Autocorrelation function of a link in the M6 over a period of 4 weeks, showing seasonal patterns on the daily and weekly periods.}
	\label{fig:acf}
\end{figure}

Given the cyclic nature of traffic, the aim was a prediction algorithm that could account for the periodic variations and endemic congestion while being resilient to fluctuations and rare events. This algorithm also should:
\begin{itemize}
	\item Be robust, mitigating the propagation of isolated events into future forecasts.
	\item Not require the use of time segmentation and be valid for regular and "special dates".
	\item Be location agnostic, the internal parameters should be set algorithmically based on the data.
	\item Have Gaussian, mean 0, uncorrelated residuals.
	\item Near flat Trend term, given the different time scales between seasonal cycles and changes in the general motorway flow.
\end{itemize}

\subsection{Naive Segmentation}
To obtain an accurate indication of the performance of the algorithm developed in the following subsections, an example of basic segmentation was coded. 
This involved a weighted combination of the training data points using uniform weights. 
In this way, for the $i-th$ minute of a week and using a training set composed of the previous of $n$ weeks, the Naive Segmentation (NS) profile is:
\begin{equation}
NS(i,n) = \sum_{\textrm{week}=1}^{n} \frac{x^i_n}{n} 
\end{equation}

\subsection{Decomposition in Background and Spikes}
During the exploratory data analysis it was found that, from the point of view of travel times, traffic operates in two clearly differentiated regimes. 
\begin{itemize}
	\item Background: 
	\begin{itemize}
		\item Stable around a mean value.
		\item Oscillates with small amplitude and high frequency.
		\item Suitable for spectral filtering.
	\end{itemize}
	\item Spikes: 
	\begin{itemize}
		\item Zero most of the time. Quickly go to extreme values.
		\item Oscillates with great amplitude and low inter-oscillation frequency.
		\item Suitable for seasonal decomposition.
	\end{itemize}
\end{itemize}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{new_flow.png}
	\caption{Schematic of data streams in the algorithm.}
	\label{fig:flowchart}
\end{figure}

In order to stop the large deviations from affecting the underlying true profile, it was decided to separate these spikes from the rest and treat them in a separate manner. 
Assuming Gaussian noise $\xi$:
\begin{equation}
Travel \textrm{ } Time_t = Background_t + Spikes_t + \xi
\end{equation}
The objective was to separate them in such a way that most of the recurring congestion is captured as part of the Background and used for spectral analysis, attempting to mitigate the prediction error induced by the high frequency oscillations and obtaining a basic view of what can be daily observed.
Meanwhile, the spikes, including the recurring component not captured in the background and the non-recurring congestion, were treated separately in an attempt to find seasonality in larger time scales than those in which the background oscillates. 
Ideally, after this step, an operator should only find isolated large rare events deviating from the profile and white noise.\\

To prevent the differing lengths of the links from affecting this decomposition, for this step, all travel times were normalized according to their corresponding link's free flow time, understood as the time to transverse the length of the link at the maximum legal speed allowed by the motorway. 
However, this step only mitigates the non-regularity of the time series, since there are drivers who do not follow these limits.\\

A threshold was heuristically chosen to separate the two components in the different regimes from the normalised travel times. 
Intuitively, this threshold scales with the amount of recurring congestion in a link. 
Whenever a data point is above the threshold, it is flagged as belonging to a spike.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{BS.pdf}
	\caption{ Decomposition of time series in background and spikes }
	\label{fig:Background}
\end{figure}

For this purpose, an indicator function was defined, taking for every minute the value:\\
\begin{equation}
\delta_t^{spike}= \left\{
\begin{array}{lr}
1 & : \textrm{Over threshold}\\
0 & : \,\,\,\,\,\,\,\,\,\,\,\,\textrm{Otherwise}
\end{array}
\right.
\label{eq:delta}
\end{equation}

\begin{table}[htbp]
	\caption{Normalised Travel Time Thresholds}
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Motorway}&{\textbf{Threshold}} \\
			\hline
			M6& 1.1\\
			\hline
			M11& 1.2\\
			\hline
			M25& 1.4\\
			\hline
		\end{tabular}
		\label{tab1}
	\end{center}
\end{table}

\subsection{Spectral Component}
The main difficulty when dealing with the Background time series is the low amplitude, high frequency fluctuations that can be found almost ubiquitously. 
In these situations signal smoothing can be easily performed by removing completely the frequency components from a certain frequency and up while the information bearing low frequency components are retained. For this task, the Fast Fourier Transform (FFT) \cite{FFT} was used.

Below, the sequence of steps taken to process the Background part of the measured travel times is described
\begin{enumerate}

	\item Calculate Background Power Spectrum using FFT
	\item Remove frequencies corresponding to periods under 4 hours and over 1 week
	\item Repeat for all $n$ weeks in training set
    \item Apply and EWMA to the weekly predictions
	\item Compute the Inverse Transform

\end{enumerate}
\subsection{Seasonality Component}
Seasonal-Trend Decomposition based on LOESS (STL) \cite{STL} was chosen for the seasonality analysis since it will handle any type of seasonality, allowing the user to control how it changes over time as well as the smoothness of the trend-cycle, being robust to outliers \cite{forecasting}.\\

Below, the sequence of steps taken to extract the Seasonality Components that can be seen in Fig. \ref{fig:acf} is described. 
Note that this should be applied to the $n$ training weeks as a single time series.
\begin{enumerate}
	\item STL Decomposition of Background for daily seasonality
	\item Extract and sum the series corresponding Trend and Remainder from Step 1
	\item STL decomposition for weekly seasonality
	\item Comprobation of Background's Remainder and Trend from Step 3
	\begin{enumerate}
		\item Background's Remainder should be zero mean, Gaussian distributed
		\item Background's Trend should have a near zero slope
	\end{enumerate}
	\item Add daily and weekly Seasonal components from Step 1 and Step 3 to obtain global seasonality
	\item Average seasonality across training weeks
	\item Linearise Trend term from
	\begin{enumerate}
		\item Select extreme values in the series and create a linear interpolation between them
		\item Extract the trend corresponding to the number of weeks for forecasting
	\end{enumerate}
	\item Add linearised trend to seasonality obtained in Step 6
	\item STL Decomposition of Spikes for weekly seasonality
	\item Comprobation of Spike's remainder and trend
	\begin{enumerate}
		\item Spike's Remainder should be zero mean, Gaussian distributed
		\item Spike's Trend should have a near zero slope
	\end{enumerate}
	\item Extract Spike's Seasonality corresponding to the number of weeks for forecasting
	\item Add Spike's Seasonal component to the output of 8) to output from 9)
\end{enumerate}
\subsection{Seasonal-Spectral Hybrid Profile}
In order to create the final Seasonal-Spectral Hybrid profile (Hybrid), one of the two forecasts generated in the previous points is taken, depending on what is the identified regime, as described in 

\begin{equation}
\textrm{Hybrid} = \textrm{Seasonal} * \delta_{\textrm{spike}} + \textrm{Spectral} * (1 - \delta_{\textrm{spike}})
\end{equation}

\section{Accuracy Results}
\begin{table*}[bp]
	\caption{MAPE Distribution Per Motorway}
	\centering
	\begin{center}
		\begin{tabular}{|C|C|C|C|C|C|C|C|C|}
			\hline
			\textbf{\% rel. error}&{\textless -25\%}&{-25\%\textbf{ to }-15\%}&{-15\%\textbf{ to }-5\%}&{-5\%\textbf{ to }5\%}&{5\%\textbf{ to }15\%}&{15 \%\textbf{ to }25\%}&{\textgreater 25\%}\\
			\hline
			M6& 1.58& 0.60& 3.77& 88.01& 5.97& 0.06& 0.01\\
			\hline
			M11& 0.80& 0.35& 4.07& 86.15& 7.97& 0.49& 0.15\\
			\hline
			M25& 3.85& 2.73& 10.42& 75.12& 7.29& 0.33& 0.28\\
			\hline
		\end{tabular}
		\label{mapeglobal}
	\end{center}
\end{table*}
  
In this section the accuracy of the algorithm described above is compared against the Published Profiles and the NS Model. 
For each temporal point $i$, the Mean Average Percentage Error (MAPE) is defined below:
\begin{equation}
\textrm{MAPE} =\frac{ \left( \sum_{i=1}^{n} \frac{\|x_i - \hat{x_i}\|}{x_i}\right)}{n}
\end{equation}
In the case of the Motorways the Root Mean Squared Error (RMSE) has been calculated for each temporal point $i$ as:
\begin{equation}
\textrm{RMSE} = \sqrt{\frac{\sum_{i=1}^{n} (x_i - \hat{x_i})^2}{n}}
\end{equation}

In both cases the error for a Link is defined as the average MAPE or RMSE across all prediction points.
The error for a Motorway is defined as the average of the error across all its links.
\subsection{Accuracy by Quantile}
\begin{figure}[htbp]
	\centerline{\includegraphics[width=\linewidth]{/finalplots/m6_q.pdf}}
	\caption{Average accuracy results in M6 across percentiles of travel time.}
	\label{fig:m6q}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{/finalplots/m11_q.pdf}
	\caption{Link average accuracy results in M11 across percentiles of travel time.}
	\label{fig:m11q}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{/finalplots/m25_q.pdf}
	\caption{Link average accuracy results in M25 across percentiles of travel time.}
	\label{fig:m25q}
\end{figure}
Here, the accuracy of the algorithm is compared against the Published Profiles and the NS Profile across all percentiles of travel time.
As it can be seen in Figures \ref{fig:m6q}, \ref{fig:m11q} and \ref{fig:m25q}, the Hybrid Profile has a higher accuracy than the Published Profiles and the NS Model for all percentiles of travel time except for the most extreme values where they all perform poorly.\\
The most meaningful difference occurs between percentiles $[50-95]$, where the Published Profile starts to suffer from higher inaccuracy.

\subsection{Daytime Error}
\begin{figure}[htbp]
	\centering
		\includegraphics[width=\linewidth]{/finalplots/m6_dt.pdf}
	\caption{Link average accuracy results in M6 across times of the day.}
	\label{fig:m6daytime}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.95\linewidth]{/finalplots/m11_dt.pdf}
	\caption{Link average accuracy results in M6 across times of the day.}
	\label{fig:m11daytime}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{/finalplots/m25_dt.pdf}
	\caption{Link average accuracy results in M25 across times of the day.}
	\label{fig:m25daytime}
\end{figure}

In this subsection, the accuracy of the algorithm is compared against the Published Profiles and the NS Profile across the times of the day.
As it can be seen in Figures \ref{fig:m6daytime}, \ref{fig:m11daytime} and \ref{fig:m25daytime}, the Hybrid Profile generated by combining the Seasonal and Spectral analysis has a higher accuracy than the Published Profiles and the NS Model for all times of the day, all locations and training lengths. 
The most relevant improvement occurs during the morning and evening peak hours, where the algorithm presented in this paper does not accuse meaningful performance worsening relative to the morning plateau when compared with the other two profiles.\\
In the case of the M6 and M11, where the training set is richer, the error at peak times is reduced by at least 50\% in all cases, reaching as much as 68.7\% in the case of the M6 morning rush.
In the case of the M25, which is congested on a regular basis, the errors in the Published Profile during peak times are slightly lower than on the other cases, indicating that, given the methods used to calculate the Published Profile, this recurrent congestion is better captured by it, without the congestion being lighter in intensity than in the cases for the M6 and M11. 
Even in this case, the proposed algorithm performs significantly better than any of the other two, except for a brief window between 6-7AM when it is outperformed by the NS Model.

\begin{table}[htbp]
	\caption{MAPE per Link on M6}
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Link}&{\textbf{MAPE}} \\
			\hline
			117007401& 2.11\%\\
			\hline
			117007501& 4.21\%\\
			\hline
			117007601& 2.46\%\\
			\hline
			117007801& 4.80\%\\
			\hline
			117007901& 3.08\%\\
			\hline
			117008401& 3.31\%\\
			\hline
			117009102& 2.69\%\\
			\hline
			117011901& 2.35\%\\
			\hline
			117012001& 4.46\%\\
			\hline
			117012101& 2.62\%\\
			\hline
			117012201& 4.31\%\\
			\hline
			117012301& 2.75\%\\
			\hline
			117016001& 3.60\%\\
			\hline
			123025901& 1.89\%\\
			\hline
		\end{tabular}
		\label{tab1}
	\end{center}
	\label{table:m6mape}
\end{table}

\begin{table}[htbp]
	\caption{Global MAPE \& RMSE per Motorway}
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			\textbf{Motorway}&{\textbf{MAPE [\%]}}&{\textbf{RMSE [s]}} \\
			\hline
			M6& 3.18\%& 0.0368\\
			\hline
			M11& 3.00\%& 0.0355\\
			\hline
			M25& 5.52\%& 0.0667\\
			\hline
		\end{tabular}
		\label{mapeglobal}
	\end{center}
\end{table}

\section{Future Work}
The algorithm presented above meets all the requirements described in Section \ref{algorithm} except that it requires a heuristically set threshold. 
One potential way of reaching total compliance with these requirements is to perform the decomposition between Background and Spikes by applying a Wavelet Transform and separating background from spikes based on statistical analysis of the transformed time series in terms of how their Wavelet coefficients oscillate over time within a scale level. 
In the future a sensitivity analysis should be conducted to explore the limits of the algorithm in terms of minimum training data set, as well as maximum performance with increased training.
\section*{Acknowledgments}
This work was part funded by the EPSRC under grant no. EP/L015374. 
We are also grateful to the London Mathematical Laboratory for additional financial support. 
We thank Ayman Boustati, Laura Guzman and Guillem Mosquera for assistance with exploratory data analysis. 
We also thank Steve Hilditch and Thales Ground Transportation Systems, UK for help understanding the NTIS system.  
A. C. E. thanks Stefan Grosskinsky for the initial guidance, discussions and support.


\begin{thebibliography}{00}
\bibitem{NTIS} The Highways Agency, "National Transport Information System Publish Services", Technical Report, 2011. 
\bibitem{STL} R. B. Cleveland, W. S. Cleveland, J. E. McRae and I. Terpenning, "STL: A Seasonal-Trend Decomposition Procedure Based on Loess", Journal of Official Statistics, vol. 6, 1990, pp. 3-73.
\bibitem{forecasting} R. J. Hyndman and G. Athanasopoulos, "Forecasting: Principles and Practice", Otexts, 2013.
\bibitem{FFT} J. W. Cooley and J. W. Tukey, "An algorithm for the machine calculation of complex Fourier series", Mathematics of computation, 1965, vol. 19, no 90, p. 297-301.

\end{thebibliography}

\end{document}
